# VCLA Autonomous Vehicles in _GTA V_

## Quick Start

Please install the provided conda environment with the following command:

```
$ conda env create -f environment.yml
```

Then, clone this [repository](https://github.com/jasonjewik/darknet). It's a fork of Darknet with some modifications for this project.

Next, download and extract this [tar.gz archive](https://drive.google.com/file/d/1ariEUydkqnxhP_eJlA5WcE3l0CcOKjrS/view?usp=sharing) - the folder contained within should go into data_labeler.
Your directory structure should look like this:

```
~> /path/to/this/repository
   |-- data_labeler
   |    |-- darknet_files
   |    |  |-- classes.names
   |    |  |-- obj.data
   |    |  |-- yolov3.cfg
   |    |  |-- yolov3_10000.weights
   |-- crop_images.py
   |-- darknet.sh
   |-- ... all the other files ...
```

To label data generated by the GTA V trainer, do the following commands:

```
$ cd data_labeler
$ conda activate data_labeler
$ python label_data.py /path/to/GTAV_program/drivedata /path/to/darknet --open
```

Results will appear in /path/to/GTAV_program/drivedata/results/.
Please see the **Contents** section for advanced usage.

## Contents

### data_labeler

- `label_data.py`

  - should be the only script you need to run
  - if something goes wrong, try running the following scripts in the order they are shown, except `graph_coords.py` and `test_results.py`
  - creates labeled pickle files, putting them into GTAV_program/drivedata/results\
  - if the --parent flag is set, the script expects the following directory structure:

    ```
    ~> /path/to/root_folder/
       |-- child_folder_1
       |   |-- *.csv
       |   |-- RGB_whole.raw
       |-- ...
       |-- child_folder_n
       |   |-- *.csv
       |   |-- RGB_whole.raw
    ```

- `generate_pickles.py`

  - creates .pkl files from GTAV_program/drivedata/RGB_whole.raw
  - puts them into GTAV_program/drivedata/pickles
  - can take a long time before the progress bar is shown if the RGB_whole.raw file is very large

- `generate_visuals.py`

  - creates .jpg and/or .avi files from GTAV_program/drivedata/pickles/\*.pkl
  - puts them into GTAV_program/drivedata/imvid

- `crop_images.py`

  - crops the .jpg files in GTAV_program/drivedata/imvid/\*.jpg
  - puts them into GTAV_program/drivedata/crop
  - assumes a motorcycle as viewed in 3rd person mode

- `invoke_darknet.py`

  - runs darknet detection on the images to check for braking

- `parse_turns.py`

  - parse the turns based on data in GTAV_program/drivedata/\*.csv and the .jpg files in GTAV_program/drivedata/crop
  - generates pkl files and puts them in GTAV_program/drivedata/results
  - the pkl files are five frames formatted as Numpy arrays, followed by an array indicating the key presses
  - key presses are formatted like \[A, D, W, S\]

- `graph_coords.py`

  - displays a graphical representation of the coordinates in GTAV_program/drivedata/\*.csv

- `test_results.py`

  - unpickles the results in GTAV_program/drivedata/results
  - uses cv2 to show the images and predicted keys

## To Do

- [x] Parse drivedata CSV files
- [x] Test on Windows
- [x] Train YOLOv3 model to determine braking/not-braking
- [x] Figure out why cv2.imshow has a GTK error on my machine
- [x] Run YOLOv3 model on cropped images to detect braking
- [x] Change `parse_turns.py` to print out keys

## Further Notes

- YOLOv3 model trained with [Darknet](https://github.com/pjreddie/darknet)
- My forked version with changes to auto-label data is [here](https://github.com/jasonjewik/darknet)
- Image annotations done with [LabelImg](https://github.com/tzutalin/labelImg)
